# Configuration for fine-tuning on handwritten datasets

base: base.yaml

# Datasets for fine-tuning (handwritten)
data:
  datasets:
    - crohme
    - mathwriting
    - hme100k

# Model - match pretrained architecture
model:
  backbone: resnet34
  d_model: 384
  dim_feedforward: 1536
  pretrained_backbone: false  # Load from pretrained checkpoint instead

# Training - lower LR for fine-tuning
training:
  batch_size: 32
  learning_rate: 5.0e-5
  num_epochs: 50
  warmup_steps: 500
  patience: 15

# Checkpoint to load from
checkpoint:
  pretrained_path: checkpoints/pretrain/best.pt
  freeze_encoder: false  # Can freeze CNN for initial epochs

# Logging
logging:
  checkpoint_dir: checkpoints/finetune
  log_dir: logs/finetune
  save_every: 10

# Configuration for pretraining on PRINTED_TEX_230k

base: base.yaml

# Dataset
data:
  datasets:
    - printed_tex
  train_ratio: 0.95

# Model - use larger capacity for pretraining
model:
  backbone: resnet34
  d_model: 384
  dim_feedforward: 1536

# Training - longer training for pretraining
training:
  batch_size: 48
  learning_rate: 3.0e-4
  num_epochs: 30
  patience: 5
  warmup_steps: 2000

# Logging
logging:
  checkpoint_dir: checkpoints/pretrain
  log_dir: logs/pretrain

